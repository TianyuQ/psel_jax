# ============================================================================
# Configuration File for Player Selection Network (PSN) Project
# ============================================================================
# This file contains all parameters and hyperparameters used across:
# - Reference trajectory generation
# - PSN training 
# - Goal inference pretraining
# - Testing and evaluation scripts
#
# CRITICAL DATA SOURCE DISTINCTION:
# ┌─────────────────────────────────────────────────────────────────────────┐
# │ TRAINING: Uses reference_trajectories_Np/ directories                  │
# │   - Goal inference model training                                      │
# │   - PSN training with pretrained goals                                 │
# │   - All model training phases use reference trajectory data            │
# │                                                                         │
# │ TESTING: Different data sources for different models                   │
# │   - Goal inference testing: reference_trajectories_Np/                 │
# │     (tests goal prediction accuracy on clean reference data)           │
# │   - PSN testing: receding_horizon_trajectories_Np/                     │
# │     (tests player selection in realistic receding horizon scenarios)   │
# └─────────────────────────────────────────────────────────────────────────┘
# ============================================================================

# ============================================================================
# GLOBAL GAME PARAMETERS
# ============================================================================
game:
  # Time discretization
  dt: 0.1                    # Time step size (seconds)
  T_receding_horizon_planning: 20            # Planning horizon for each individual game
  T_receding_horizon_iterations: 50               # Total number of receding horizon iterations
  T_total: 50               # Total number of time steps in trajectory (T_receding_horizon_planning * T_receding_horizon_iterations)
  T_observation: 10         # Number of steps to observe before solving the game

  # Agent configuration
  N_agents: 4               # Total number of agents in the game
  ego_agent_id: 0           # ID of the ego agent (usually 0)
  
  # State and control dimensions
  state_dim: 4              # [x, y, vx, vy] for point mass agents
  control_dim: 2            # [ax, ay] for point mass agents
  
  # Environment parameters
  # Boundary sizes are auto-determined: ±2.5m for ≤4 agents, ±3.5m for >4 agents

# ============================================================================
# OPTIMIZATION PARAMETERS
# ============================================================================
optimization:
  # iLQGames solver parameters
  num_iters: 50            # Number of iterations for iLQGames solver  
  step_size: 0.005          # Step size for gradient descent
  
  # Cost function weights
  navigation_weight: 1.0    # Weight for navigation cost
  collision_weight: 1.0    # Weight for collision avoidance
  collision_scale: 1.0      # Scale factor in exponential collision cost
  control_weight: 0.1       # Weight for control cost

# ============================================================================
# REFERENCE TRAJECTORY GENERATION
# ============================================================================
reference_generation:
  # Data generation parameters
  num_samples: 512           # Number of reference trajectory samples to generate
  save_dir: "reference_trajectories_${N_agents}p"  # Directory to save reference trajectories
  
  # Data split configuration
  train_samples: 384         # Number of samples to use for training (first N samples)
  test_samples: 128          # Number of samples to use for testing (later N samples)
  
  # Trajectory type
  trajectory_type: "linear" # Type of reference trajectory ["linear", "spline", "polynomial"]
  
  # Visualization
  save_plots: true          # Whether to save trajectory plots
  plot_format: "png"        # Plot format ["png", "pdf", "svg"]
  plot_dpi: 300             # Plot resolution

# ============================================================================
# GOAL INFERENCE NETWORK PARAMETERS
# ============================================================================
goal_inference:
  # Network architecture
  # Hidden layer dimensions: [128, 64, 32] for 4 players, [256, 128, 64] for 10 players
  hidden_dims_4p: [64, 32]          # Hidden dimensions for MLP head (reduced for GRU version)
  hidden_dims_10p: [128, 64]        # Hidden dimensions for MLP head (reduced for GRU version)
  gru_hidden_size: 64               # GRU hidden size for temporal processing
  activation: "relu"        # Activation function ["relu", "tanh", "swish"]
  dropout_rate: 0.3         # Dropout rate during training (increased for GRU version)
  
  # Training parameters
  learning_rate: 0.001      # Learning rate for Adam optimizer
  batch_size: 32            # Batch size for training
  num_epochs: 100          # Number of training epochs
  
  # Loss function weights
  goal_loss_weight: 1.0     # Weight for goal prediction loss
  regularization_weight: 0.001  # L2 regularization weight
  
  # Finetuning configuration
  finetune_with_game_solving: true  # Enable finetuning with receding horizon game solving
  finetune_start_epoch_ratio: 0.8   # Start finetuning at 50% of total epochs
  finetune_learning_rate: 0.001    # Reduced learning rate for finetuning
  similarity_loss_weight: 1.0       # Weight for similarity loss vs prediction loss
  freeze_gru_layers: true           # Freeze GRU layers during finetuning
  freeze_early_mlp_layers: true     # Freeze early MLP layers, keep last layer trainable
  
  # Data parameters
  observation_length: 10    # Length of observation history (K_observation)
  
  # Early stopping
  patience: 50              # Patience for early stopping
  min_delta: 1e-4          # Minimum change for early stopping

# ============================================================================
# PLAYER SELECTION NETWORK PARAMETERS
# ============================================================================
psn:
  # Network architecture
  # Hidden layer dimensions: [64, 32] for 4 players, [128, 64] for 10 players (reduced for GRU version)
  hidden_dims_4p: [64, 32]          # Hidden dimensions for MLP head (reduced for GRU version)
  hidden_dims_10p: [128, 64]        # Hidden dimensions for MLP head (reduced for GRU version)
  gru_hidden_size: 64               # GRU hidden size for temporal processing
  output_dim_per_agent: 8   # Output dimension per other agent
  activation: "relu"        # Activation function
  use_batch_norm: false     # Disabled for GRU version (use dropout instead)
  dropout_rate: 0.3         # Dropout rate (increased for GRU version)
  
  # Training parameters
  learning_rate: 0.002      # Learning rate for Adam optimizer
  batch_size: 32            # Batch size for training
  num_epochs: 100            # Number of training epochs
  
  # Loss function weights
  sigma1: 0.075              # Weight for mask sparsity loss
  sigma2: 0.05               # Weight for mask diversity loss
  
  # Goal source configuration
  # IMPORTANT: This setting affects testing behavior:
  # - If true: PSN training uses true goals, testing should also use true goals
  # - If false: PSN training uses goal inference model, testing should also use goal inference model
  # The testing script will automatically detect this setting to ensure consistency
  use_true_goals: true     # Whether to use true goals instead of predicted goals from goal inference model
  
  # Mask parameters
  mask_threshold: 0.5       # Threshold for converting continuous to binary mask
  temperature: 1.0          # Temperature for Gumbel-Softmax
  hard_gumbel: false        # Whether to use hard Gumbel-Softmax
  
  # Scheduler parameters
  scheduler_factor: 0.5     # Factor for learning rate reduction
  scheduler_patience: 10    # Patience for learning rate scheduler

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  # Data source for training
  # IMPORTANT: Training should ALWAYS use reference trajectories
  data_source: "reference_trajectories"     # Use reference trajectories for training
  data_dir: "reference_trajectories_${N_agents}p"  # Directory containing reference trajectory data
  
  # Data split configuration
  train_samples: 384         # Number of samples to use for training (first N samples)
  test_samples: 128          # Number of samples to use for testing (later N samples)
  
  # General training settings
  seed: 42                  # Random seed for reproducibility
  use_gpu: true             # Whether to use GPU if available
  mixed_precision: false    # Whether to use mixed precision training
  
  # Data loading
  num_workers: 4            # Number of data loading workers
  prefetch_factor: 2        # Prefetch factor for data loading
  
  # Logging and monitoring
  log_interval: 10          # Interval for logging training metrics
  eval_interval: 50         # Interval for evaluation during training
  save_interval: 100        # Interval for saving checkpoints
  
  # TensorBoard logging
  tensorboard_log: true     # Whether to enable TensorBoard logging
  log_dir: "log"            # Base directory for logs
  
  # Model saving
  save_best_only: true      # Whether to save only the best model
  model_save_format: "pkl"  # Format for saving models ["pkl", "flax"]

# ============================================================================
# TESTING AND EVALUATION
# ============================================================================
testing:
  # Data source for testing - Different for different models
  # GOAL INFERENCE: Uses reference trajectories (tests goal prediction accuracy)
  # PSN: Uses receding horizon trajectories (tests player selection in realistic scenarios)
  
  # Goal inference testing data
  goal_inference_data_source: "reference_trajectories"
  goal_inference_data_dir: "reference_trajectories_${N_agents}p"  # Goal inference tests on reference trajectories
  
  # PSN testing data  
  psn_data_source: "receding_horizon_trajectories"
  psn_data_dir: "receding_horizon_trajectories_${N_agents}p"  # PSN tests on receding horizon trajectories
  
  # Data split configuration (shared across all testing)
  train_samples: 384         # Number of samples to use for training (first N samples)
  test_samples: 128          # Number of samples to use for testing (later N samples)
  
  # Legacy/general testing parameters (for backward compatibility)
  data_source: "receding_horizon_trajectories"  # Default for PSN testing
  data_dir: "receding_horizon_trajectories_${N_agents}p"  # Default directory
  
  # Test data parameters
  num_test_samples: 10      # Number of test samples to evaluate
  test_data_dir: "receding_horizon_trajectories_${N_agents}p"  # Directory with test data (DEPRECATED: use specific data_dir instead)
  
  # Model paths for testing
  # These are template paths that will be dynamically filled based on actual parameters
  # Format: log/goal_inference_gru_N_{N_agents}_T_{T_total}_obs_{T_observation}_lr_{learning_rate}_bs_{batch_size}_goal_loss_weight_{goal_loss_weight}_epochs_{num_epochs}/
  psn_model_template: "log/goal_inference_gru_N_{N_agents}_T_{T_total}_obs_{T_observation}_lr_{goal_inference_lr}_bs_{batch_size}_goal_loss_weight_{goal_loss_weight}_epochs_{num_epochs}/psn_gru_pretrained_goals_N_{N_agents}_T_{T_total}_obs_{T_observation}_lr_{psn_lr}_bs_{batch_size}_sigma1_{sigma1}_sigma2_{sigma2}_epochs_{psn_epochs}/psn_final_model.pkl"
  goal_inference_model_template: "log/goal_inference_rh_gru_N_{N_agents}_T_{T_total}_obs_{T_observation}_lr_{learning_rate}_bs_{batch_size}_goal_loss_weight_{goal_loss_weight}_epochs_{num_epochs}/goal_inference_best_model.pkl"
  
  # Default model paths (fallback for when templates can't be resolved)
  psn_model: null  # Will be set programmatically based on actual training parameters
  goal_inference_model: null  # Will be set programmatically based on actual training parameters
  
  # Receding horizon testing parameters
  receding_horizon:
    # Two-phase approach configuration
    initial_stabilization_iterations: 0  # No stabilization period - use PSN and goal inference from step 10
    mask_threshold: 0.5                  # Threshold for agent selection (0.05 -> 0.3)
    mask_sparsity_calculation: "fraction" # How to calculate mask sparsity ["fraction", "ratio"]
    
    # Goal source configuration for testing
    # Choose between true goals and goal inference for PSN testing:
    # - "true_goals": Use ground truth goals for PSN testing, save to receding_horizon_results_goal_true
    # - "goal_inference": Use goal inference model predictions, save to receding_horizon_results_goal_inference
    goal_source: "goal_inference"            # ["true_goals", "goal_inference"]
    
    # Goal inference input method (only used when goal_source is "goal_inference")
    # - "first_steps": Use first 10 steps (observation period) as input
    # - "sliding_window": Use sliding window of latest 10 steps as input (more realistic)
    goal_inference_input_method: "first_steps"  # ["first_steps", "sliding_window"]
    
    # Testing configuration
    num_samples: 10                      # Number of samples to test (2 -> 10)
    output_dir: "receding_horizon_test_results"  # Test results will be placed under the PSN model directory
    
    # Visualization settings
    create_gif: true                     # Whether to create trajectory GIFs
    gif_duration: 0.5                    # Duration per frame in seconds
    gif_loop: 0                          # Number of loops (0 = infinite)
    
    # Performance monitoring
    track_computation_time: true         # Whether to track game solving time
    save_detailed_results: true          # Whether to save detailed iteration results
  
  # Evaluation metrics
  compute_trajectory_error: true    # Whether to compute trajectory tracking error
  compute_collision_rate: true      # Whether to compute collision statistics
  compute_goal_accuracy: true       # Whether to compute goal prediction accuracy
  
  # Visualization
  create_animations: true           # Whether to create trajectory animations
  save_comprehensive_plots: true   # Whether to save detailed analysis plots
  animation_fps: 10                 # FPS for animations
  
  # Output directories
  results_dir: "test_results"       # Directory for test results
  plots_dir: "test_plots"           # Directory for test plots

# ============================================================================
# UTILITY FUNCTIONS FOR MODEL PATHS
# ============================================================================
# These functions can be implemented in your code to automatically resolve model paths
# based on the current configuration parameters

# Example Python function to resolve model paths:
# def get_model_paths(config):
#     """Get model paths based on current configuration."""
#     psn_model_path = config.testing.psn_model_template.format(
#         N_agents=config.game.N_agents,
#         T_total=config.game.T_total,
#         T_observation=config.game.T_observation,
#         learning_rate=config.goal_inference.learning_rate,
#         batch_size=config.goal_inference.batch_size,
#         goal_loss_weight=config.goal_inference.goal_loss_weight,
#         num_epochs=config.goal_inference.num_epochs,
#         sigma1=config.psn.sigma1,
#         sigma2=config.psn.sigma2,
#         psn_epochs=config.psn.num_epochs
#     )
#     
#     goal_inference_model_path = config.testing.goal_inference_model_template.format(
#         N_agents=config.game.N_agents,
#         T_total=config.game.T_total,
#         T_observation=config.goal_inference.observation_length,  # Use observation_length, not game.T_observation
#         learning_rate=config.goal_inference.learning_rate,
#         batch_size=config.goal_inference.batch_size,
#         goal_loss_weight=config.goal_inference.goal_loss_weight,
#         num_epochs=config.goal_inference.num_epochs
#     )
#     
#     return psn_model_path, goal_inference_model_path

# ============================================================================
# PATHS AND DIRECTORIES
# ============================================================================
paths:
  # Data directories
  # TRAINING DATA: Always use reference trajectories for training all models
  reference_data_dir: "reference_trajectories_${N_agents}p"           # For reference trajectory generation
  training_data_dir: "reference_trajectories_${N_agents}p"            # For goal inference and PSN training
  goal_inference_data_dir: "reference_trajectories_${N_agents}p"      # Goal inference model training data
  
  # TESTING DATA: Different for different models
  goal_inference_testing_data_dir: "reference_trajectories_${N_agents}p"      # Goal inference testing (tests goal prediction)
  psn_testing_data_dir: "receding_horizon_trajectories_${N_agents}p"          # PSN testing (tests player selection)
  testing_data_dir: "receding_horizon_trajectories_${N_agents}p"              # Default/legacy testing directory
  
  # Data split configuration (shared across all data loading)
  train_samples: 384         # Number of samples to use for training (first N samples)
  test_samples: 128          # Number of samples to use for testing (later N samples)

  # Model directories
  models_dir: "models"
  checkpoints_dir: "checkpoints"
  
  # Output directories
  results_dir: "results"
  logs_dir: "log"
  plots_dir: "plots"

# ============================================================================
# DEBUGGING AND DEVELOPMENT
# ============================================================================
debug:
  # Debug mode settings
  gradient_clip_value: 1.0  # Value for gradient clipping

# ============================================================================
# DEVICE CONFIGURATION
# ============================================================================
device:
  # Device selection
  preferred_device: "gpu"   # Preferred device ["gpu", "cpu", "auto"]
  
  # JAX configuration
  jax_platform_name: null  # JAX platform name (null = auto-detect)

# ============================================================================
# DYNAMIC CONFIGURATION HELPERS
# ============================================================================
# These values are automatically computed based on N_agents
# Access them in your code as: config.goal_inference.hidden_dims_4p or config.goal_inference.hidden_dims_10p
# Then select based on N_agents:
#   hidden_dims = config.goal_inference.hidden_dims_4p if config.game.N_agents == 4 else config.goal_inference.hidden_dims_10p

# Model path templates can be resolved programmatically:
#   psn_model_path = config.testing.psn_model_template.format(
#       N_agents=config.game.N_agents,
#       T_total=config.game.T_total,
#       T_observation=config.goal_inference.observation_length,  # Use observation_length, not game.T_observation
#       learning_rate=config.goal_inference.learning_rate,
#       batch_size=config.goal_inference.batch_size,
#       goal_loss_weight=config.goal_inference.goal_loss_weight,
#       num_epochs=config.goal_inference.num_epochs,
#       sigma1=config.psn.sigma1,
#       sigma2=config.psn.sigma2,
#       psn_epochs=config.psn.num_epochs
#   )
